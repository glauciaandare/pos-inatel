A complexidade irredutível abaixo da qual um sinal não pode ser comprimido é conhecida como **entropia** do sinal, um conceito fundamental na teoria da informação.

### Entropia

**Entropia** é uma medida da quantidade média de informação contida em uma fonte de dados. Em termos simples, a entropia representa a menor quantidade de bits necessária, em média, para codificar cada símbolo de uma fonte de dados sem perda de informação.

Claude Shannon introduziu a entropia na sua obra seminal em 1948. A fórmula para a entropia \(H(X)\) de uma variável aleatória \(X\) com \(n\) possíveis valores \(\{x_1, x_2, ..., x_n\}\) e respectivas probabilidades \(\{p_1, p_2, ..., p_n\}\) é dada por:

\[ H(X) = -\sum_{i=1}^{n} p_i \log_2 p_i \]

### Interpretação da Entropia

1. **Limite Inferior de Compressão**: A entropia de uma fonte de dados define o limite teórico inferior para qualquer algoritmo de compressão sem perdas. Nenhum algoritmo pode, em média, comprimir os dados a uma taxa menor que a entropia da fonte.

2. **Incerteza**: A entropia mede a incerteza associada a uma variável aleatória. Quanto maior a incerteza, maior a entropia. Para uma variável aleatória perfeitamente previsível (como uma constante), a entropia é zero.

### Exemplos

- **Fonte Binária Justa**: Para uma fonte que gera bits 0 e 1 com igual probabilidade (0,5 cada), a entropia é:

  \[ H(X) = - (0,5 \log_2 0,5 + 0,5 \log_2 0,5) = 1 \text{ bit} \]

  Isso significa que, em média, um bit é necessário para representar cada bit gerado pela fonte.

- **Fonte Não Uniforme**: Se uma fonte gera o bit 0 com probabilidade 0,9 e o bit 1 com probabilidade 0,1, a entropia é:

  \[ H(X) = - (0,9 \log_2 0,9 + 0,1 \log_2 0,1) \approx 0,469 \text{ bits} \]

  Isso indica que, em média, menos de um bit é necessário para representar cada bit gerado pela fonte devido à alta previsibilidade.

### Aplicações

- **Compressão de Dados**: Algoritmos de compressão, como Huffman Coding e Arithmetic Coding, visam aproximar-se do limite de entropia para reduzir o tamanho dos dados sem perda de informação.
- **Criptografia**: A entropia também é relevante na criptografia, onde uma alta entropia é desejável para garantir a imprevisibilidade das chaves criptográficas.
- **Comunicação**: Na comunicação digital, a entropia é usada para determinar a eficiência da codificação de dados e para calcular a capacidade de canais de comunicação.

### Conclusão

A entropia é a medida fundamental que define a complexidade irredutível de um sinal. Ela estabelece o limite teórico para a compressão de dados sem perdas e é crucial para o desenvolvimento de algoritmos de compressão eficientes e sistemas de comunicação robustos.