## A Teroia da informação fornece uma modelagem matemática, que  permite responder 2 questões fundamentais.

### 1 - A complexidade irredutível abaixo da qual um sinal não pode ser comprimido? 

Ela é conhecida como **entropia** do sinal, um conceito fundamental na teoria da informação.

### Entropia

**Entropia** é uma medida da quantidade média de informação contida em uma fonte de dados. Em termos simples, a entropia representa a menor quantidade de bits necessária, em média, para codificar cada símbolo de uma fonte de dados sem perda de informação.

Claude Shannon introduziu a entropia na sua obra seminal em 1948. A fórmula para a entropia **H(X)** de uma variável aleatória **(X)** com (n) possíveis valores  ${ x_1, x_2, ..., x_n }$  e respectivas probabilidades ${ p1, p2, ..., pn }$ é dada por:

$$
H(X) = -\sum_{i=1}^{n} p_i \log_2 p_i
$$

### Interpretação da Entropia

1. **Limite Inferior de Compressão**: A entropia de uma fonte de dados define o limite teórico inferior para qualquer algoritmo de compressão sem perdas. Nenhum algoritmo pode, em média, comprimir os dados a uma taxa menor que a entropia da fonte.

2. **Incerteza**: A entropia mede a incerteza associada a uma variável aleatória. Quanto maior a incerteza, maior a entropia. Para uma variável aleatória perfeitamente previsível (como uma constante), a entropia é zero.

### Exemplos

- **Fonte Binária Justa**: Para uma fonte que gera bits 0 e 1 com igual probabilidade (0,5 cada), a entropia é:

  $$
  H(X) = - (0,5 \log_2 0,5 + 0,5 \log_2 0,5) = 1{bit} 
  $$

  Isso significa que, em média, um bit é necessário para representar cada bit gerado pela fonte.

- **Fonte Não Uniforme**: Se uma fonte gera o bit 0 com probabilidade 0,9 e o bit 1 com probabilidade 0,1, a entropia é:

 
 $$ 
  H(X) = - (0,9 \log_2 0,9 + 0,1 \log_2 0,1) \approx 0,469{ bits}
 $$
 
  Isso indica que, em média, menos de um bit é necessário para representar cada bit gerado pela fonte devido à alta previsibilidade.

### Aplicações

- **Compressão de Dados**: Algoritmos de compressão, como Huffman Coding e Arithmetic Coding, visam aproximar-se do limite de entropia para reduzir o tamanho dos dados sem perda de informação.
- **Criptografia**: A entropia também é relevante na criptografia, onde uma alta entropia é desejável para garantir a imprevisibilidade das chaves criptográficas.
- **Comunicação**: Na comunicação digital, a entropia é usada para determinar a eficiência da codificação de dados e para calcular a capacidade de canais de comunicação.

### Conclusão

A entropia é a medida fundamental que define a complexidade irredutível de um sinal. Ela estabelece o limite teórico para a compressão de dados sem perdas e é crucial para o desenvolvimento de algoritmos de compressão eficientes e sistemas de comunicação robustos.

### 2 - QUAL É A MAXIMA TAXA DE TRANSMISSÃO PARA UMA COMUNICAÇÃO CONFIAVEL EM UM CANAL RUIDOSO?

A máxima taxa de transmissão para uma comunicação confiável em um canal ruidoso é conhecida como **capacidade do canal**. Ela foi definida por Claude Shannon em seu teorema fundamental da teoria da informação. A capacidade do canal é a taxa máxima na qual a informação pode ser transmitida por um canal com uma taxa arbitrariamente baixa de erro, desde que técnicas de codificação adequadas sejam utilizadas.

Para um canal com ruído aditivo branco gaussiano (AWGN), a capacidade \( C \) pode ser calculada usando a fórmula:


> $$
C = B \log_2 \left( 1 + \frac{P}{N_0 B} \right)
$$



onde:
- \( C \) é a capacidade do canal em bits por segundo (bps),
- \( B \) é a largura de banda do canal em hertz (Hz),
- \( P \) é a potência média do sinal transmitido,
- \( N_0 \) é a densidade espectral de potência do ruído (em watts por hertz).

Em palavras simples, esta fórmula nos diz que a capacidade do canal aumenta com a largura de banda e com a relação sinal-ruído.

> A expressão é $\frac{P}{N_0 B}$.


